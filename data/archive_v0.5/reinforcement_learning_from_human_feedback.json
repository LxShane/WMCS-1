{
  "name": "Reinforcement Learning from Human Feedback",
  "type": "ABSTRACT_CONCEPT",
  "id": {
    "group": 50,
    "item": 1238
  },
  "claims": [
    {
      "predicate": "USED_FOR",
      "object": "optimizing a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments",
      "temporal": {
        "valid_from": "ORIGIN",
        "valid_until": "PRESENT",
        "event": null
      },
      "epistemic": {
        "confidence": 0.8,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "USED_FOR",
      "object": "aligning model outputs with user expectations",
      "temporal": {
        "valid_from": "ORIGIN",
        "valid_until": "PRESENT",
        "event": null
      },
      "epistemic": {
        "confidence": 0.8,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "USED_FOR",
      "object": "improving factuality",
      "temporal": {
        "valid_from": "ORIGIN",
        "valid_until": "PRESENT",
        "event": null
      },
      "epistemic": {
        "confidence": 0.8,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "USED_FOR",
      "object": "reducing harmful responses",
      "temporal": {
        "valid_from": "ORIGIN",
        "valid_until": "PRESENT",
        "event": null
      },
      "epistemic": {
        "confidence": 0.8,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "USED_FOR",
      "object": "enhancing task performance",
      "temporal": {
        "valid_from": "ORIGIN",
        "valid_until": "PRESENT",
        "event": null
      },
      "epistemic": {
        "confidence": 0.8,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    }
  ],
  "surface_layer": {
    "definition": "Reinforcement learning from human feedback (RLHF) applies reinforcement learning methods to optimize a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments.",
    "common_uses": []
  },
  "deep_layer": {},
  "instance_layer": {},
  "facets": {},
  "epistemic_metadata": {
    "source": "Set Automatically",
    "citations": [
      "https://en.wikipedia.org/wiki/Large_language_model"
    ]
  }
}