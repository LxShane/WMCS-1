{
  "name": "Transformer Architecture",
  "type": "ABSTRACT_CONCEPT",
  "id": {
    "group": 50,
    "item": 1236
  },
  "claims": [
    {
      "predicate": "REPLACED",
      "object": "recurrence",
      "temporal": {
        "valid_from": "2017",
        "valid_until": "PRESENT",
        "event": "Introduction of Transformer Architecture"
      },
      "epistemic": {
        "confidence": 0.9,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "USES",
      "object": "self-attention",
      "temporal": {
        "valid_from": "2017",
        "valid_until": "PRESENT",
        "event": "Introduction of Transformer Architecture"
      },
      "epistemic": {
        "confidence": 0.9,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "ENABLES",
      "object": "efficient parallelization",
      "temporal": {
        "valid_from": "2017",
        "valid_until": "PRESENT",
        "event": "Introduction of Transformer Architecture"
      },
      "epistemic": {
        "confidence": 0.9,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "ENABLES",
      "object": "longer context handling",
      "temporal": {
        "valid_from": "2017",
        "valid_until": "PRESENT",
        "event": "Introduction of Transformer Architecture"
      },
      "epistemic": {
        "confidence": 0.9,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    },
    {
      "predicate": "ENABLES",
      "object": "scalable training on unprecedented data volumes",
      "temporal": {
        "valid_from": "2017",
        "valid_until": "PRESENT",
        "event": "Introduction of Transformer Architecture"
      },
      "epistemic": {
        "confidence": 0.9,
        "status": "SETTLED",
        "source_type": "ACADEMIC"
      }
    }
  ],
  "surface_layer": {
    "definition": "The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes."
  },
  "deep_layer": {},
  "instance_layer": {},
  "facets": {},
  "epistemic_metadata": {
    "source": "Set Automatically",
    "citations": [
      "https://en.wikipedia.org/wiki/Large_language_model"
    ]
  }
}