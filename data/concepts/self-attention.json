{
  "name": "Self-Attention",
  "type": "ABSTRACT_CONCEPT",
  "id": {
    "group": 50,
    "item": 1237
  },
  "claims": [],
  "surface_layer": {
    "definition": "Self-attention is a component of the transformer architecture."
  },
  "deep_layer": {},
  "instance_layer": {},
  "facets": {},
  "epistemic_metadata": {
    "source": "Set Automatically",
    "citations": [
      "https://en.wikipedia.org/wiki/Large_language_model"
    ]
  }
}